## Section 6: Verification

### Automatically Repairing Network Control Planes Using an Abstract Representation（泛读）
由于网络控制层（control plane）的设置非常复杂：不同路由之间有很多不同的协议，且路由之间不是互相孤立的，修改一个路由设置可能会引入新的问题。论文希望能提供了一种证明和修复的方法：抽象出整个控制层的带层次结构有向图，确定相邻路由和同一个路径上路由间协议和设置的相关性，按照确定算法遍历得到约束。同时为了使得修改的部分最小，额外添加约束。最后转化成一个MAX-SMT问题（NP-Hard），用z3求解。

> 比较新颖的内容应该是HARC，简而言之，是网络控制层的specification的有向图表示，对于抽象出一个specification有一定的借鉴意义。  
> 另外，求解SMT依然是相似的，但是和FS不同的是，每一个policy都有不同的要求，需要依此添加约束，这里的思路也是蛮值得借鉴的。  
> specification的抽象尽量小的同时，应该还是要同时兼顾到完备性。Hyperkernel的状态机和这里的有向图实际上是相似的，都是为了尽量的完备地描述状态（路径）变化。

### Hyperkernel: Push-Button Verification of an OS Kernel (精读)
作者希望将之前用在文件系统上的证明方法重用在OS内核上。针对系统的复杂性，做了以下几件事：
- 简化接口实现，加入对于接口的额外规定：finitized
- 使用llvm中间表达（IR）进行证明，而非C语言
- 简化系统内存管理实现

同时还额外要求系统是单处理器关中断的，对于其他的也有额外的要求。

论文给出了他认为较好的specification表述：一个状态机用于描述调用如何工作，一个更为高级的自然语义表述便于人理解。

证明过程依然类似之前的工作，用python描述specification，用Z3求解，给出证明或者testcase。

> 这个工作是不是作偏了？从希望系统证明变为不断修改系统本身来满足求解器和证明过程的不足。  
> 就我对CSAIL这一系列程序证明的工作来看，最直接的方法还是给出一个well-defined and appropriate specification.

## Section 8: Adaption and Repair
### Drizzle: Fast and Adaptable Stream Processing at Scale（泛读）
大规模流处理系统（Large scale streaming processing system），例如spark，hadoop，需要同时满足高吞吐量，低延迟，对于错误和工作量变化的快速反应等等需求。
此前的工作在这三者上往往有所取舍。作者基于**流任务需要毫秒级的处理延迟，而工作量和集群的变化率是分钟级的，远低于任务延迟需求**这一发现，降低了容错和自适应需要的同步率（？翻译的很乱但是意思我是明白的）。最后在spark上实现了更低的延迟，更快的故障恢复速度。
> 类似DCTCP，这个工作分析workload和cluster的特性，设计算法对于变化进行反馈，进而消除不必要的overhead。其中关于dataplane和batch的使用其实可以多了解一下。
> 算是scalability类型的问题。
